# Importing our data
library(readr)
mydata <- read_delim("C:/Users/mehdi/Desktop/Projects/15 - Statistics/Logistic regression/logistic regression dataset-Social_Network_Ads.txt", 
                     delim = "\t", escape_double = FALSE, 
                     trim_ws = TRUE)

# 1. Data preprocessing
# Checking whether our categorical variables are factors:
class(mydata$Gender)
class(mydata$Purchased)

mydata$Gender <- as.factor(mydata$Gender)
mydata$Purchased <- as.factor(mydata$Purchased)

class(mydata$Gender)
class(mydata$Purchased)

# Determining reference level:
mydata$Gender <- relevel(mydata$Gender , ref = "Male")
mydata$Purchased <- relevel(mydata$Purchased , ref = "0")


# 2. Logistic regression:
library(carData)
library(car)
model <- glm(Purchased ~ 1 + Gender + Age + EstimatedSalary, data = mydata , family = binomial())
model
summary(model)

# Checking the fitness of our model:
modelchi <- model$null.deviance - model$deviance
modelchi

modeldf <- model$df.null - model$df.residual
modeldf

modelprob <- 1 - pchisq(modelchi , modeldf)
modelprob

# Checking the R2 of our model:
logisticPseudoR2s <- function(LogModel) {
  dev <- LogModel$deviance
  nullDev <- LogModel$null.deviance
  modelN <- length(LogModel$fitted.values)
  R.l <- 1 - dev / nullDev
  R.cs <- 1- exp ( -(nullDev - dev) / modelN)
  R.n <- R.cs / ( 1 - ( exp (-(nullDev / modelN))))
  cat("Pseudo R^2 for logistic regression\n")
  cat("Hosmer and Lemeshow R^2 ", round(R.l, 3), "\n")
  cat("Cox and Snell R^2 ", round(R.cs, 3), "\n")
  cat("Nagelkerke R^2 ", round(R.n, 3), "\n")
}

logisticPseudoR2s(model)

# Checking the odds ratios of our model variables:
exp(model$coefficients)
exp(confint(model))

# Using our model to predict:
customer <- data.frame(Gender = "Female" , Age = 10 , EstimatedSalary = 120000)
customerprob <- predict(model , customer , type = "response")
customerprob


# Creating a plot for our logistic regression model:
class(mydata$Purchased)

mydata$Purchased <- as.numeric(mydata$Purchased) - 1

class(mydata$Purchased)

library(ggplot2)

# Plotting:
plot <- ggplot(data = mydata, mapping = aes(x = Age, y = Purchased, color = "")) +
  geom_jitter(height = 0.05, alpha = 0.8, size = 1.5, shape = 16) + 
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = TRUE, color = 'blue') +
  theme_classic() +
  labs(title = "Logistic regression plot",
       x = "Age",
       y = "Purchase status",
       color = "Legend") +
  theme(plot.title = element_text(size = 12, face = 'bold'),
        axis.title.x = element_text(size = 10),
        axis.title.y = element_text(size = 10),
        axis.text.x = element_text(size = 10),
        legend.title = element_text(size = 10, face = 'bold'),
        legend.position = 'right') + 
  scale_color_manual(values = c('black') , labels = c('Participants')) +
  scale_y_continuous(labels = c('Not purchased' , 'Purchased'), breaks = c(0 , 1))

plot


# Casewise diagnostics: Outlier
# Standardized residuals:
mydata$standresid <- rstandard(model)

# Studentized residuals:
mydata$studresid <- rstudent(model)

subset(mydata, standresid > 1.96)
subset(mydata, standresid < -1.96)

# Casewise diagnostics: Influential cases

mydata$DFFit <- dffits(model)
mydata$DFFit > 1

DFBeta <- dfbeta(model)
DFBeta > 1

mydata$leverage <- hatvalues(model)
# Expected value: (k  + 1)/N  k = Number of predictors  N = number of observations
# Expected value: (3 + 1)/400 = 4/400 = 0.01
hist(mydata$leverage)


# Checking the linearity assumption of the logit

mydata$logAgeInt <- log(mydata$Age) * mydata$Age
mydata$logESInt <-log(mydata$EstimatedSalary) * mydata$EstimatedSalary

model2 <- glm(Purchased ~ Age + Gender + EstimatedSalary + logAgeInt + logESInt,
              data = mydata, family = binomial())
summary(model2)
